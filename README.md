# SAMRL

# -*- coding: utf-8 -*-
"""ogb_tfrecord

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MzE4GpwloM7YYLtoDL7R4fZDN5WAV3Xz
"""

!pip install ogb

!wget https://storage.googleapis.com/hien7613storage2/datasets/ogbl_wikikg2.zip

!unzip ogbl_wikikg2.zip

"""# Import"""

import os
import time
from time import sleep
from tqdm import tqdm
import psutil
import tensorflow as tf
from collections import namedtuple
from ogb.linkproppred import LinkPropPredDataset
from torch.utils.data import Dataset

# from dataloader import DataLoader
# from dataloader import DataGenerator
# from dataloader import DataGenerator2Dataset, BidirectionalOneShotIterator, Bidirectional2Dataset
# from model import TFKGEModel

class TestDataset(Dataset):
    def __init__(self, triples, args, mode, random_sampling):
        self.len = len(triples['head'])
        self.triples = triples
        self.nentity = args['nentity']
        self.nrelation = args['nrelation']
        self.mode = mode
        self.random_sampling = random_sampling
        if random_sampling:
            self.neg_size = args['neg_size_eval_train']

    def __len__(self):
        return self.len

    def __getitem__(self, idx):
        head, relation, tail = self.triples['head'][idx], self.triples['relation'][idx], self.triples['tail'][idx]
        positive_sample = torch.LongTensor((head, relation, tail))

        if self.mode == 'head-batch':
            if not self.random_sampling:
                negative_sample = torch.cat([torch.LongTensor([head]), torch.from_numpy(self.triples['head_neg'][idx])])
            else:
                negative_sample = torch.cat(
                    [torch.LongTensor([head]), torch.randint(0, self.nentity, size=(self.neg_size,))])
        elif self.mode == 'tail-batch':
            if not self.random_sampling:
                negative_sample = torch.cat([torch.LongTensor([tail]), torch.from_numpy(self.triples['tail_neg'][idx])])
            else:
                negative_sample = torch.cat(
                    [torch.LongTensor([tail]), torch.randint(0, self.nentity, size=(self.neg_size,))])

        return positive_sample, negative_sample, self.mode

class TrainDataset(Dataset):
    def __init__(self, triples, nentity, nrelation, negative_sample_size, mode):
        self.len = len(triples)
        self.triples = triples
        self.triple_set = set(triples)
        self.nentity = nentity
        self.nrelation = nrelation
        self.negative_sample_size = negative_sample_size
        self.mode = mode
        self.count = self.count_frequency(triples)
        self.true_head, self.true_tail = self.get_true_head_and_tail(self.triples)

    def __len__(self):
        return self.len

    def __getitem__(self, idx):
        positive_sample = self.triples[idx]

        head, relation, tail = positive_sample

        subsampling_weight = self.count[(head, relation)] + self.count[(tail, -relation-1)]
        subsampling_weight = torch.sqrt(1 / torch.Tensor([subsampling_weight]))

        negative_sample_list = []
        negative_sample_size = 0

        while negative_sample_size < self.negative_sample_size:
            negative_sample = np.random.randint(self.nentity, size=self.negative_sample_size*2)
            if self.mode == 'head-batch':
                mask = np.in1d(
                    negative_sample,
                    self.true_head[(relation, tail)],
                    assume_unique=True,
                    invert=True
                )
            elif self.mode == 'tail-batch':
                mask = np.in1d(
                    negative_sample,
                    self.true_tail[(head, relation)],
                    assume_unique=True,
                    invert=True
                )
            else:
                raise ValueError('Training batch mode %s not supported' % self.mode)
            negative_sample = negative_sample[mask]
            negative_sample_list.append(negative_sample)
            negative_sample_size += negative_sample.size

        negative_sample = np.concatenate(negative_sample_list)[:self.negative_sample_size]

        negative_sample = torch.LongTensor(negative_sample)

        positive_sample = torch.LongTensor(positive_sample)

        return positive_sample, negative_sample, subsampling_weight, self.mode

    @staticmethod
    def collate_fn(data):
        positive_sample = torch.stack([_[0] for _ in data], dim=0)
        negative_sample = torch.stack([_[1] for _ in data], dim=0)
        subsample_weight = torch.cat([_[2] for _ in data], dim=0)
        mode = data[0][3]
        return positive_sample, negative_sample, subsample_weight, mode

    @staticmethod
    def count_frequency(triples, start=4):
        '''
        Get frequency of a partial triple like (head, relation) or (relation, tail)
        The frequency will be used for subsampling like word2vec
        '''
        count = {}
        for head, relation, tail in triples:
            if (head, relation) not in count:
                count[(head, relation)] = start
            else:
                count[(head, relation)] += 1

            if (tail, -relation-1) not in count:
                count[(tail, -relation-1)] = start
            else:
                count[(tail, -relation-1)] += 1
        return count

    @staticmethod
    def get_true_head_and_tail(triples):
        '''
        Build a dictionary of true triples that will
        be used to filter these true triples for negative sampling
        '''

        true_head = {}
        true_tail = {}

        for head, relation, tail in triples:
            if (head, relation) not in true_tail:
                true_tail[(head, relation)] = []
            true_tail[(head, relation)].append(tail)
            if (relation, tail) not in true_head:
                true_head[(relation, tail)] = []
            true_head[(relation, tail)].append(head)

        for relation, tail in true_head:
            true_head[(relation, tail)] = np.array(list(set(true_head[(relation, tail)])))
        for head, relation in true_tail:
            true_tail[(head, relation)] = np.array(list(set(true_tail[(head, relation)])))

        return true_head, true_tail



!mv /content/content/dataset/ogbl_wikikg2 /content

def read_triple(file_path, entity2id, relation2id):
    triples = []
    with open(file_path) as fin:
        for line in fin:
            h, r, t = line.strip().split('\t')
            triples.append((entity2id[h], relation2id[r], entity2id[t]))
    return triples


def read_data():
    with open(os.path.join(data_path, 'entities.dict')) as fin:
        entity2id = dict()
        for line in fin:
            eid, entity = line.strip().split('\t')
            entity2id[entity] = int(eid)

    with open(os.path.join(data_path, 'relations.dict')) as fin:
        relation2id = dict()
        for line in fin:
            rid, relation = line.strip().split('\t')
            relation2id[relation] = int(rid)

    train_triples = read_triple(os.path.join(data_path, 'train.txt'), entity2id, relation2id)
    valid_triples = read_triple(os.path.join(data_path, 'valid.txt'), entity2id, relation2id)
    test_triples = read_triple(os.path.join(data_path, 'test.txt'), entity2id, relation2id)
    return train_triples, valid_triples, test_triples, entity2id, relation2id


def get_split_dict():
  print('LinkPropPredDataset')
  dataset = LinkPropPredDataset(name = 'ogbl-wikikg2', root='/content/')
  print('get_edge_split')
  split_dict = dataset.get_edge_split()
  return dataset, split_dict

def run_main(dataset, split_dict):
    # train_triples, valid_triples, test_triples, entity2id, relation2id = read_data()
    # nentity = len(entity2id)
    # nrelation = len(relation2id)

    print('DataGenerator')
    nentity = dataset.graph['num_nodes']
    nrelation = int(max(dataset.graph['edge_reltype'])[0])+1
    train_triples = split_dict['train']
    valid_triples = split_dict['valid']
    test_triples = split_dict['test']
    args = {}
    args['nentity'] = nentity
    args['nrelation'] = nrelation
    args['neg_size_eval_train'] = 500
    random_sampling = True
    negative_sample_size = 256
    # print(train_triples.keys())
    # print(valid_triples.keys())
    # print(test_triples.keys())
    # all_true_triples = [train_triples + valid_triples + test_triples]

    # # train
    train_dataloader_head = TrainDataset(train_triples, nentity, nrelation, negative_sample_size, 'head-batch')
    train_dataloader_tail = TrainDataset(train_triples, nentity, nrelation, negative_sample_size, 'tail-batch')

    # train_generator_head = DataGenerator(train_triples, nentity, nrelation, negative_sample_size, 0, 'head-batch')
    # train_generator_tail = DataGenerator( train_triples, nentity, nrelation, negative_sample_size, 1, 'tail-batch')

    # train_dataset_head, train_length_head = DataGenerator2Dataset().convert(data_generator=train_generator_head)
    # train_dataset_tail, train_length_tail = DataGenerator2Dataset().convert(data_generator=train_generator_tail)



    # # Prepare dataloader for evaluation
    # test_dataloader_head = TestDataset(valid_triples, args,
    #                                               'head-batch',
    #                                               random_sampling)

    # test_dataloader_tail = TestDataset(valid_triples, args,
    #                                               'tail-batch',
    #                                               random_sampling)


    return train_dataloader_head, train_dataloader_tail

# dataset, split_dict = get_split_dict()

dataloader_head, dataloader_tail = run_main(dataset, split_dict)

# next(iter(test_dataloader_head))

dataloader_tail.len

import torch
# train_data_iter = iter(test_dataloader_head)

def serialize_example(tensor1, tensor2, tensor3, tensor4):
    feature = {
        'positive_sample': tf.train.Feature(int64_list=tf.train.Int64List(value=tensor1.numpy().flatten())),
        'negative_sample': tf.train.Feature(int64_list=tf.train.Int64List(value=tensor2.numpy().flatten())),
        'subsampling_weight': tf.train.Feature(float_list=tf.train.FloatList(value=tensor3.numpy().flatten())),
        'mode': tf.train.Feature(int64_list=tf.train.Int64List(value=[tensor4.numpy().tolist()])),
    }
    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
    return example_proto.SerializeToString()

# output_file = '/content/ogbl_head_valid.tfrecord'
output_file = '/content/ogbl_tail_valid.tfrecord'
with tf.io.TFRecordWriter(output_file) as writer:
    for tensor1, tensor2, tensor4 in tqdm(dataloader_tail, total=429456):
      tensor3 = torch.tensor(1.)
      if tensor4 == 'head-batch':
        tensor4 = torch.tensor(0)
      else:
        tensor4 = torch.tensor(1)
      serialized_data = serialize_example(tensor1, tensor2, tensor3, tensor4)
      writer.write(serialized_data)

!gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp  '/content/ogbl_tail_valid.tfrecord' gs://hien7613storage2/datasets/KGE/ogbl-wikikg2/valid/

"""# nnnnnnnnnnnnn"""



data_path = "data/wn18rr"
import os

def read_triple(file_path, entity2id, relation2id):
    triples = []
    with open(file_path) as fin:
        for line in fin:
            h, r, t = line.strip().split('\t')
            triples.append((entity2id[h], relation2id[r], entity2id[t]))
    return triples


def read_data():
    with open(os.path.join(data_path, 'entities.dict')) as fin:
        entity2id = dict()
        for line in fin:
            eid, entity = line.strip().split('\t')
            entity2id[entity] = int(eid)

    with open(os.path.join(data_path, 'relations.dict')) as fin:
        relation2id = dict()
        for line in fin:
            rid, relation = line.strip().split('\t')
            relation2id[relation] = int(rid)

    train_triples = read_triple(os.path.join(data_path, 'train.txt'), entity2id, relation2id)
    valid_triples = read_triple(os.path.join(data_path, 'valid.txt'), entity2id, relation2id)
    test_triples = read_triple(os.path.join(data_path, 'test.txt'), entity2id, relation2id)
    return train_triples, valid_triples, test_triples, entity2id, relation2id



train_triples, valid_triples, test_triples, entity2id, relation2id = read_data()
nentity = len(entity2id)
nrelation = len(relation2id)

all_true_triples = train_triples + valid_triples + test_triples

len(test_triples)

next(iter(test_dataloader_head))[2].shape

from dataloader import TestDataset
from torch.utils.data import DataLoader

test_dataloader_head = DataLoader(
    TestDataset(
        test_triples,
        all_true_triples,
        nentity,
        nrelation,
        'tail-batch'
    ),
    batch_size=1,
    num_workers=max(1, 8 // 2),
    collate_fn=TestDataset.collate_fn
)

import tensorflow as tf
import numpy as np
def serialize_example(tensor1, tensor2, tensor3, tensor4):
    feature = {
        'positive_sample': tf.train.Feature(int64_list=tf.train.Int64List(value=tensor1)),
        'negative_sample': tf.train.Feature(int64_list=tf.train.Int64List(value=tensor2.flatten())),
        'subsampling_weight': tf.train.Feature(float_list=tf.train.FloatList(value=tensor3.flatten())),
        'mode': tf.train.Feature(int64_list=tf.train.Int64List(value=[tensor4])),
    }
    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
    return example_proto.SerializeToString()

output_file = f'/content/wn18rr_test_tail.tfrecord'
with tf.io.TFRecordWriter(output_file) as writer:
    for tensor1, tensor2, tensor3, tensor4 in tqdm(test_dataloader_head, total=3134):
        tensor1 = tensor1[0]
        tensor2 = tensor2.numpy()
        tensor3 = tensor3.numpy()
        tensor4 = 1
        # print(tensor1)
        serialized_data = serialize_example(tensor1, tensor2, tensor3, tensor4)
        writer.write(serialized_data)

!gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp /content/wn18rr_test_tail.tfrecord gs://hien7613storage2/datasets/KGE/

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install ogb

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/CustomKnowledgeGraphEmbedding/codes_tf

!python -u /content/CustomKnowledgeGraphEmbedding/codes_tf/run.py

import numpy as np

def serialize_example(tensor1, tensor2, tensor3, tensor4):
    feature = {
        'tensor1': tf.train.Feature(int64_list=tf.train.Int64List(value=np.array(tensor1))),
        'tensor2': tf.train.Feature(int64_list=tf.train.Int64List(value=tensor2.flatten())),
        'tensor3': tf.train.Feature(float_list=tf.train.FloatList(value=tensor3.flatten())),
        'tensor4': tf.train.Feature(int64_list=tf.train.Int64List(value=[tensor4])),
    }
    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))
    return example_proto.SerializeToString()

output_file = f'/content/ogbl_head_{k_part}.tfrecord'
with tf.io.TFRecordWriter(output_file) as writer:
    for tensor1, tensor2, tensor3, tensor4 in tqdm(data, total=len_part):
        serialized_data = serialize_example(tensor1, tensor2, tensor3, tensor4)
        writer.write(serialized_data)

!gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp /content/drive/MyDrive/wn18rr_head.tfrecord gs://hien7613storage2/datasets/KGE/
!gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp /content/drive/MyDrive/wn18rr_tail.tfrecord gs://hien7613storage2/datasets/KGE/
